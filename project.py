# -*- coding: utf-8 -*-
"""cv_proj_II.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16jNor6VII1V5ix5hn2PTpKxnLgTch-N-
"""
import tensorflow as tf
from tensorflow import keras
from tensorflow.python.framework import dtypes
from tensorflow.keras import datasets, Model
from tensorflow.keras import backend as K
from tensorflow.keras.initializers import glorot_uniform
from tensorflow.keras.layers import Input, Reshape, RepeatVector,Conv2D, Concatenate, MaxPool2D, Flatten, Dense
import copy
import numpy as np
import os
import cv2
import pathlib
import matplotlib
import matplotlib.pyplot as plt

output_path = "output"
if not os.path.isdir(output_path):
    os.mkdir(output_path)
    #exit(0)
#To use GPU/GPU
physicalDevices = tf.config.list_physical_devices('GPU')  
#tf.config.experimental.set_memory_growth(physical_devices[0], True) # Set if memory growth should be enabled for a PhysicalDevice
np.set_printoptions(suppress=True)  # suppress scientific notation

#PREPARING THE TRAINING AND TEST DATA
# Loads the MNIST dataset
mnist = tf.keras.datasets.mnist

# https://www.tensorflow.org/api_docs/python/tf/keras/datasets/mnist/load_data
(x_train, label_train), (x_test, label_test) = mnist.load_data()

train = x_train[:4000, :, :]
trainLabel = label_train[:4000]
numTrain = train .shape[0]

validation = x_train[4000:5000, :, :]
validationLabel = label_train[4000:5000]
numValidation = validation.shape[0]

test = x_test[:1000, :, :]
testLabel = label_test[:1000]
numTest = test.shape[0]

# One-hot encode the training
yTrain = np.zeros([numTrain, 10])
for i in range(numTrain):
    yTrain[i, trainLabel[i]] = 1

# One-hot encode validation
yValidation = np.zeros([numValidation, 10])
for i in range(numValidation):
    yValidation[i, validationLabel[i]] = 1

# One-hot encode the testing
yTest = np.zeros([numTest, 10])
for i in range(numTest):
    yTest[i, testLabel[i]] = 1

print("Train.shape", train.shape)
print("Validation.shape", validation.shape)
print("Test.shape", test.shape)
print("trainLabel.shape", trainLabel.shape)
print("validationLabel.shape", validationLabel.shape)
print("testLabel.shape", testLabel.shape)
print("yTrain.shape", yTrain.shape)
print("yValidation.shape", yValidation.shape)
print("yTest.shape", yTest.shape)

#Saving first 5 images for testing
output_path = "output"
for i in range(5):
  filename = "train_"+str(i)+".png"
  path = os.path.join(output_path,filename)
  cv2.imwrite(path, train[i])

for i in range(5):
  filename = "validation_"+str(i)+".png"
  path = os.path.join(output_path,filename)
  cv2.imwrite(path, validation[i])

for i in range(5):
  filename = "test_"+str(i)+".png"
  path = os.path.join(output_path,filename)
  cv2.imwrite(path, test[i])

#Normalizing the data
train, validation, test = train/255.0, validation/255.0, test/255.0

#ResNet5 creation
kernel_size, stride = 3, 2

def resNet5():
  ip = Input(shape=[784])
  RVector = RepeatVector(2)(ip)
  reshape = Reshape(target_shape=[28, 28, -1])(RVector)
  
  conv2d_1 = Conv2D(filters=2, kernel_size=3,
                            padding='same', activation='relu')(reshape)
  conv2d_2 = Conv2D(filters=2, kernel_size=3,
                            padding='same', activation='relu')(conv2d_1)
  concatenate_1 = Concatenate()([reshape, conv2d_2])
  maxpool_1 = MaxPool2D(pool_size=(2, 2),
                                strides=(2, 2))(concatenate_1)
  
  conv2d_3 = Conv2D(filters=4, kernel_size=3,
                            padding='same', activation='relu')(maxpool_1)
  conv2d_4 = Conv2D(filters=4, kernel_size=3,
                            padding='same', activation='relu')(conv2d_3)
  concatenate_2 = Concatenate()([maxpool_1, conv2d_4])
  maxpool_2 = MaxPool2D(pool_size=(2, 2),
                                strides=(2, 2))(concatenate_2)
  
  conv2d_5 = Conv2D(filters=8, kernel_size=3,
                            padding='same', activation='relu')(maxpool_2)
  conv2d_6 = Conv2D(filters=8, kernel_size=3,
                            padding='same', activation='relu')(conv2d_5)
  concatenate_3 = Concatenate()([maxpool_2, conv2d_6])
  maxpool_3 = MaxPool2D(pool_size=(2, 2),
                                strides=(2, 2))(concatenate_3)
  
  conv2d_7 = Conv2D(filters=16, kernel_size=3,
                                 padding='same', activation='relu')(maxpool_3)
  conv2d_8 = Conv2D(filters=16, kernel_size=3,
                            padding='same', activation='relu')(conv2d_7)
  concatenate_4 = Concatenate()([maxpool_3, conv2d_8])
  maxpool_4 = MaxPool2D(pool_size=(2, 2),
                                strides=(2, 2))(concatenate_4)
  conv2d_9 = Conv2D(filters=32, kernel_size=3,
                            padding='same', activation='relu')(maxpool_4)
  conv2d_10 = Conv2D(filters=32, kernel_size=3,
                            padding='same', activation='relu')(conv2d_9)

  flatten = Flatten()(conv2d_10)

  dense_1 = Dense(20, activation="softmax")(flatten)
  dense_2 = Dense(10, activation="softmax")(dense_1)

  model = Model(inputs= ip, outputs=dense_2, name='Resnet5')
  return model

train_model = resNet5()

model = Model(inputs=train_model.input, outputs=train_model.output)
model.summary()

optimizer = keras.optimizers.Adam(learning_rate=0.002)

newTrain = np.reshape(train, (-1, 28*28))
newValidation = np.reshape(validation, (-1, 28*28))
newTest = np.reshape(test, (-1, 28*28))

trainDataset = tf.data.Dataset.from_tensor_slices((newTrain, yTrain))
trainDataset = trainDataset.batch(100)

valueDataset = tf.data.Dataset.from_tensor_slices((newValidation, yValidation))
valueDataset = valueDataset.batch(100)

def custom_loss(yTrain, yPrediction):
    yTrain = tf.cast(yTrain, dtype=dtypes.float32)                                       
    yPrediction = tf.cast(yPrediction, dtype=dtypes.float32)                                       
    loss = tf.math.multiply(- (1./yTrain.shape[0]), tf.math.reduce_sum(tf.math.multiply(yTrain, tf.math.log(yPrediction))))
    return loss

def get_correct_predicted_instances(yTrain, yPrediction):
    yTrain = K.argmax(yTrain)
    yPrediction = K.argmax(yPrediction)

    accuracy = tf.reduce_sum(K.cast(K.equal(yTrain, yPrediction), dtype=K.floatx()))
    return accuracy

epochs = 150
trainingLossData = np.empty(epochs)
validationLossData = np.empty(epochs)
trainingMetricsAccuracy = np.empty(epochs)
validationMetricsAccuracy = np.empty(epochs)
testMetricsAccuracy = np.empty(epochs)

for epoch in range(epochs):
    lossForEpoch = 0
    correctedTrainInstance = 0
    for step, (xTrainData, yTrainData) in enumerate(trainDataset):
        with tf.GradientTape() as tape:
            yPredictedData = model(xTrainData, training=True)
            valueForLoss = custom_loss(yTrainData, yPredictedData)
            valueForLoss1 = tf.math.reduce_sum(valueForLoss)
        grads = tape.gradient(valueForLoss, model.trainable_weights)
        optimizer.apply_gradients(zip(grads, model.trainable_weights))
        lossForEpoch = lossForEpoch + valueForLoss1

        correct_predicted_instances = get_correct_predicted_instances(yTrainData, yPredictedData)
        correctedTrainInstance += correct_predicted_instances
    lossForEpoch = lossForEpoch/4000
    trainingLossData[epoch] = lossForEpoch
    trainingMetricsAccuracy[epoch] = correctedTrainInstance/4000

    # Display metrics at the end of each epoch.
    print("Training accuracy on epoch {}: {}-------------------------".format(epoch, trainingMetricsAccuracy[epoch]))

    lossForEpoch,totalCorrectValueInstance = 0,0
    for xValueData, yValueData in valueDataset:
        yPredictedData = model(xValueData, training=False)
        valueForLoss = custom_loss(yValueData, yPredictedData)
        valueForLoss = tf.math.reduce_sum(valueForLoss)
        lossForEpoch = lossForEpoch + valueForLoss
        correctValidationInstance = get_correct_predicted_instances(yValueData, yPredictedData)
        totalCorrectValueInstance = totalCorrectValueInstance + correctValidationInstance
    lossForEpoch = lossForEpoch/1000
    validationLossData[epoch] = lossForEpoch
    valueAccuracy = totalCorrectValueInstance/1000
    validationMetricsAccuracy[epoch] = valueAccuracy
    print("Validation accuracy on epoch {} is :  {}".format(epoch, valueAccuracy))

yPrediction = model(newTest, training=False)
valueForLoss = custom_loss(yPrediction, yTest)
valueForLoss = tf.math.reduce_sum(valueForLoss)
correctValidationInstance = get_correct_predicted_instances(yPrediction, yTest)
lossForEpoch = valueForLoss
testAccuracy = totalCorrectValueInstance/1000
testMetricsAccuracy = valueAccuracy
print("Accuracy of testing is  {}".format(testAccuracy))

print("Loss array for training is :   \n")
print(trainingLossData)
print("Loss array for validation is :   \n")
print(validationLossData)
print("Accuracy array for training is :   \n")
print(trainingMetricsAccuracy)
print("Accuracy array validation is :   \n")
print(validationMetricsAccuracy)
print("Accuracy array for testing is :   \n")
print(testMetricsAccuracy)

plot_x = range(1, epochs+1)
plt.title("ResNet-4 Accuracy")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.plot(plot_x, trainingMetricsAccuracy, label='Train Accuracy', color="skyblue")
plt.plot(plot_x, validationMetricsAccuracy,label='Validation Accuracy', color="orange")
plt.plot(plot_x, np.repeat(testAccuracy.numpy(), epochs), label='Test Accuracy', color="purple")
plt.legend()
plt.savefig("accuracy.png")

plot_x = range(1, epochs+1)
plt.title("ResNet-4 Loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.plot(plot_x, trainingLossData, label='Train Loss', color="skyblue")
plt.plot(plot_x, validationLossData,label='Validation Loss', color="orange")
plt.legend()
plt.savefig("loss.png")

"""
Versions
Python: 3.9.12
TensorFlow: 2.8
OpenCV: 4.5.5

References:
https://www.tensorflow.org/api_docs/python/tf/keras/datasets/mnist/load_data
https://www.tensorflow.org/datasets/catalog/mnist
https://www.tensorflow.org/datasets/keras_example

"""